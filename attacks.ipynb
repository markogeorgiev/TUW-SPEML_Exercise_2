{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing of the data: ",
   "id": "515f271a6d0e1229"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-24T09:49:22.171409Z",
     "start_time": "2025-07-24T09:49:21.639412Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "FOLDER_PATH = \"dataset/MIA_SDG_Exercise\"\n",
    "SYNTHETIC_FILES = [f\"synthetic_data{i}.csv\" for i in range(1, 5)]\n",
    "TEST_FILES = [\"test_data_with_outliers.csv\", \"test_data_wto_outliers.csv\"]\n",
    "LABEL_COLUMN = \"is_member\"  # Only in test datasets\n",
    "\n",
    "def load_dataset(path, drop_label=False):\n",
    "    df = pd.read_csv(path)\n",
    "    if drop_label and LABEL_COLUMN in df.columns:\n",
    "        df = df.drop(columns=[LABEL_COLUMN])\n",
    "    return df\n",
    "\n",
    "def build_preprocessing_pipeline(df):\n",
    "    categorical_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "    numeric_cols = df.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "    # Treat 'readmission_status' as categorical if misclassified\n",
    "    if \"readmission_status\" in numeric_cols:\n",
    "        numeric_cols.remove(\"readmission_status\")\n",
    "        categorical_cols.append(\"readmission_status\")\n",
    "\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-999)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"cat\", cat_pipe, categorical_cols),\n",
    "        (\"num\", num_pipe, numeric_cols)\n",
    "    ])\n",
    "\n",
    "    return preprocessor, categorical_cols + numeric_cols\n",
    "\n",
    "def preprocess_pair(synth_df, test_df):\n",
    "    # Find overlapping columns only\n",
    "    shared_columns = [col for col in test_df.columns if col in synth_df.columns]\n",
    "    synth_df = synth_df[shared_columns]\n",
    "    test_df = test_df[shared_columns]\n",
    "\n",
    "    # Build preprocessing pipeline on test set\n",
    "    preprocessor, _ = build_preprocessing_pipeline(test_df)\n",
    "\n",
    "    X_test = preprocessor.fit_transform(test_df)\n",
    "    X_synth = preprocessor.transform(synth_df)\n",
    "\n",
    "    return X_test, X_synth\n",
    "\n",
    "def preprocess_all():\n",
    "    results = {}\n",
    "    for synth_file in SYNTHETIC_FILES:\n",
    "        synth_path = os.path.join(FOLDER_PATH, synth_file)\n",
    "        synth_df = load_dataset(synth_path)\n",
    "\n",
    "        for test_file in TEST_FILES:\n",
    "            test_path = os.path.join(FOLDER_PATH, test_file)\n",
    "            test_df = load_dataset(test_path, drop_label=True)\n",
    "\n",
    "            key = (synth_file, test_file)\n",
    "            try:\n",
    "                X_test, X_synth = preprocess_pair(synth_df.copy(), test_df.copy())\n",
    "                results[key] = (X_test, X_synth)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to preprocess {key}: {e}\")\n",
    "\n",
    "    return results  # dict[(synth_file, test_file)] = (X_test, X_synth)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = preprocess_all()\n",
    "    print(f\"✅ Preprocessed {len(data)} dataset pairs successfully.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed 8 dataset pairs successfully.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attack 1\n",
    "\n",
    "### **1. How it uses the given synthetic data? How does it process it?**\n",
    "\n",
    "The synthetic data files (`synthetic_data1.csv` to `synthetic_data4.csv`) are generated samples meant to mimic real-world private data. These files are used to test if an attacker can **reverse-engineer membership**—i.e., guess if a specific record from the original dataset was used during training.\n",
    "\n",
    "#### Processing steps:\n",
    "\n",
    "* Each synthetic file is **loaded** using `pd.read_csv`.\n",
    "* The corresponding **test files** (`test_data_with_outliers.csv` and `test_data_wto_outliers.csv`) are also loaded. These test files contain actual data samples, some of which are labeled as **`is_member = yes`** (they were used during synthetic data training), and others as **`is_member = no`** (they were not).\n",
    "* The script **aligns the columns** between synthetic and test datasets to ensure a fair comparison.\n",
    "* For each record in the test data (row-by-row), the script performs two attacks by comparing the test row to the synthetic data distribution:\n",
    "\n",
    "  * Mode Collapse Attack\n",
    "  * Conditional Imbalance Attack\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Against what does it compare the synthetic data metrics to make predictions?**\n",
    "\n",
    "The **goal** is to compute **anomaly scores** for each test row to determine if it was part of the training data (member) or not.\n",
    "\n",
    "For each test row, the comparison is made between:\n",
    "\n",
    "#### 🧪 **The test record itself** (1 row of real data)\n",
    "\n",
    "vs.\n",
    "🎲 **The full synthetic dataset** (multiple rows generated by the model)\n",
    "\n",
    "The attack does **not** compare test rows directly against each other. Instead, it:\n",
    "\n",
    "* Takes a **single test record**, computes some statistics,\n",
    "* Compares those statistics to the **entire synthetic dataset**, and\n",
    "* Derives a **difference score** (how “unusual” the test row is relative to the synthetic distribution).\n",
    "\n",
    "The **intuition**: if a record is *too similar* to the synthetic distribution, it likely influenced the model ⇒ **member**. If it differs a lot, it probably wasn’t seen by the model ⇒ **non-member**.\n",
    "\n",
    "These scores are then used to classify each test record as member/non-member by thresholding (median is used as default threshold), and attack performance is measured using:\n",
    "\n",
    "* Accuracy, Precision, Recall, F1, AUC, ASR (Attack Success Rate).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Extremely simple overview of the attack**\n",
    "\n",
    "This attack answers the question:\n",
    "\n",
    "> \"Can I tell if a specific real person’s data was used to train this synthetic data generator?\"\n",
    "\n",
    "#### In simpler terms:\n",
    "\n",
    "* You have fake data generated by a model (synthetic).\n",
    "* You also have real data, but you're not sure which real samples were used to generate the fake ones.\n",
    "* For each real sample, you:\n",
    "\n",
    "  1. Check **how well it fits** the synthetic data.\n",
    "  2. If it fits **too well**, it's likely in the training data.\n",
    "  3. If it looks **too different**, it's probably not in the training data.\n",
    "\n",
    "By repeating this across many samples and measuring how accurate your guesses are, you perform a **membership inference attack**.\n",
    "\n",
    "---\n",
    "\n",
    "### Attack Details\n",
    "\n",
    "#### **Attack 1: Mode Collapse**\n",
    "\n",
    "* For each **categorical column**, it compares:\n",
    "\n",
    "  * The **entropy** (diversity) and **distinct ratio** (how many unique values) of that column in the **synthetic dataset** and the **single test row**.\n",
    "  * If the test row’s categorical features have unusually low diversity compared to the synthetic data (or too similar), that’s suspicious.\n",
    "  * This detects **mode collapse** — a phenomenon where generative models output only a few repeated modes (e.g., same provider or diagnosis).\n",
    "\n",
    "#### **Attack 2: Conditional Imbalance**\n",
    "\n",
    "* For certain **condition columns** (e.g., `drug_013`, `hba1c_result`) and **numeric targets** (e.g., `lab_test_count`, `stay_duration_days`):\n",
    "\n",
    "  * It finds **rare values** (values that occur <20% in the real test data).\n",
    "  * Then it compares the **distribution** of the numeric targets **conditioned on those rare values** using the **KS-test** (statistical test for distribution difference).\n",
    "  * If the conditional distribution of a test row matches the synthetic data well (or differs a lot), it might indicate its membership."
   ],
   "id": "32c0e5c864695e4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T09:27:41.886507Z",
     "start_time": "2025-07-24T09:27:41.644718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ─── Configuration ──────────────────────────────────────────────────────────────\n",
    "FOLDER_PATH = \"dataset/MIA_SDG_Exercise\"\n",
    "SYNTHETIC_FILES = [f\"synthetic_data{i}.csv\" for i in range(1, 5)]\n",
    "TEST_FILES = [\"test_data_with_outliers.csv\", \"test_data_wto_outliers.csv\"]\n",
    "LABEL_COLUMN = \"is_member\"\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"secondary_diagnosis\",\n",
    "    \"provider_specialty\",\n",
    "    \"insurance_type_code\",\n",
    "]\n",
    "\n",
    "# ─── Mode Collapse Attack ────────────────────────────────────────────────────────\n",
    "def compute_mode_collapse_scores(\n",
    "    synth_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each row in test_df, compute a mode-collapse score:\n",
    "      score_i = sum over cat-features f of |P_synth(f=value_i) - P_test(f=value_i)|.\n",
    "    Returns an array of length len(test_df).\n",
    "    \"\"\"\n",
    "    # 1) Precompute the categorical marginals\n",
    "    P_synth = {\n",
    "        col: synth_df[col].value_counts(normalize=True).to_dict()\n",
    "        for col in CATEGORICAL_FEATURES\n",
    "        if col in synth_df.columns\n",
    "    }\n",
    "    P_test = {\n",
    "        col: test_df[col].value_counts(normalize=True).to_dict()\n",
    "        for col in CATEGORICAL_FEATURES\n",
    "        if col in test_df.columns\n",
    "    }\n",
    "\n",
    "    # 2) Score each test row\n",
    "    scores = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        s = 0.0\n",
    "        # Only consider features present in both distributions\n",
    "        for col in P_synth.keys() & P_test.keys():\n",
    "            v = row[col]\n",
    "            ps = P_synth[col].get(v, 0.0)\n",
    "            pt = P_test[col].get(v, 0.0)\n",
    "            s += abs(ps - pt)\n",
    "        scores.append(s)\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def evaluate_attack(\n",
    "    scores: np.ndarray,\n",
    "    labels: pd.Series,\n",
    "    threshold: float = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Given per-row scores and true labels ('yes'/'no'), compute metrics.\n",
    "    If threshold is None, use the median of the scores.\n",
    "    \"\"\"\n",
    "    y_scores = np.array(scores)\n",
    "    y_true = np.array([1 if l == \"yes\" else 0 for l in labels])\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = np.median(y_scores)\n",
    "\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(y_true, y_scores),\n",
    "        \"ASR\": float((y_pred == y_true).mean()),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_mode_collapse_attack() -> dict:\n",
    "    \"\"\"\n",
    "    Runs the fixed Mode Collapse attack across all synthetic vs. test dataset pairs.\n",
    "    Returns a dict mapping \"synth.csv vs test.csv\" -> metrics dict.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for synth_file in SYNTHETIC_FILES:\n",
    "        synth_path = os.path.join(FOLDER_PATH, synth_file)\n",
    "        synth_df = pd.read_csv(synth_path)\n",
    "\n",
    "        for test_file in TEST_FILES:\n",
    "            test_path = os.path.join(FOLDER_PATH, test_file)\n",
    "            test_df = pd.read_csv(test_path)\n",
    "\n",
    "            key = f\"{synth_file} vs {test_file}\"\n",
    "            print(f\"🔍 Running Mode Collapse Attack on: {key}\")\n",
    "\n",
    "            # Align columns between synthetic and test (drop label)\n",
    "            common = [\n",
    "                c for c in test_df.columns\n",
    "                if c in synth_df.columns and c != LABEL_COLUMN\n",
    "            ]\n",
    "            test_subset = test_df[common + [LABEL_COLUMN]].dropna(subset=[LABEL_COLUMN])\n",
    "            synth_subset = synth_df[common].copy()\n",
    "\n",
    "            # Extract labels and feature-only DataFrame\n",
    "            labels = test_subset[LABEL_COLUMN]\n",
    "            X_test = test_subset.drop(columns=[LABEL_COLUMN])\n",
    "\n",
    "            # Compute per-row mode-collapse scores\n",
    "            scores = compute_mode_collapse_scores(synth_subset, X_test)\n",
    "\n",
    "            # Evaluate\n",
    "            metrics = evaluate_attack(scores, labels)\n",
    "            results[key] = metrics\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results: dict):\n",
    "    \"\"\"Nicely prints the attack metrics.\"\"\"\n",
    "    for pair, metrics in results.items():\n",
    "        print(f\"\\n==== Results for {pair} ====\")\n",
    "        for name, val in metrics.items():\n",
    "            print(f\"{name:>10}: {val:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    attack_results = run_mode_collapse_attack()\n",
    "    print_results(attack_results)"
   ],
   "id": "1c84bef2385216ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Mode Collapse Attack on: synthetic_data1.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Mode Collapse Attack on: synthetic_data1.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Mode Collapse Attack on: synthetic_data2.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Mode Collapse Attack on: synthetic_data2.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Mode Collapse Attack on: synthetic_data3.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Mode Collapse Attack on: synthetic_data3.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Mode Collapse Attack on: synthetic_data4.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Mode Collapse Attack on: synthetic_data4.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.4375\n",
      " Precision: 0.4375\n",
      "    Recall: 0.4375\n",
      "        F1: 0.4375\n",
      "       AUC: 0.4416\n",
      "       ASR: 0.4375\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5500\n",
      " Precision: 0.5500\n",
      "    Recall: 0.5500\n",
      "        F1: 0.5500\n",
      "       AUC: 0.5388\n",
      "       ASR: 0.5500\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.3958\n",
      " Precision: 0.3958\n",
      "    Recall: 0.3958\n",
      "        F1: 0.3958\n",
      "       AUC: 0.3898\n",
      "       ASR: 0.3958\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5500\n",
      " Precision: 0.5500\n",
      "    Recall: 0.5500\n",
      "        F1: 0.5500\n",
      "       AUC: 0.4913\n",
      "       ASR: 0.5500\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.4167\n",
      " Precision: 0.4167\n",
      "    Recall: 0.4167\n",
      "        F1: 0.4167\n",
      "       AUC: 0.4143\n",
      "       ASR: 0.4167\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5500\n",
      " Precision: 0.5500\n",
      "    Recall: 0.5500\n",
      "        F1: 0.5500\n",
      "       AUC: 0.4888\n",
      "       ASR: 0.5500\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.4792\n",
      " Precision: 0.4792\n",
      "    Recall: 0.4792\n",
      "        F1: 0.4792\n",
      "       AUC: 0.4798\n",
      "       ASR: 0.4792\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5500\n",
      " Precision: 0.5500\n",
      "    Recall: 0.5500\n",
      "        F1: 0.5500\n",
      "       AUC: 0.5062\n",
      "       ASR: 0.5500\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attack 2\n",
    "\n",
    "This script performs a **Conditional Imbalance Membership Inference Attack** (MIA), specifically targeting synthetic datasets. Its goal is to determine whether a specific test record was used to train the synthetic data generator (i.e., whether the record is a **member** of the training data). Below is a detailed breakdown of how the attack works, following your requested structure.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. How it uses the given synthetic data? How does it process it?**\n",
    "\n",
    "The synthetic data files (`synthetic_data1.csv` to `synthetic_data4.csv`) represent artificially generated data that mimics real data distributions.\n",
    "\n",
    "The processing steps are as follows:\n",
    "\n",
    "* Each synthetic file is **loaded** using pandas (`pd.read_csv`).\n",
    "* The **test files** (`test_data_with_outliers.csv`, `test_data_wto_outliers.csv`) are also loaded. These contain real samples, some labeled with `is_member = yes` (used in training the generator) and others with `is_member = no` (not used).\n",
    "* For each test record, the attack:\n",
    "\n",
    "  * Drops the `is_member` label.\n",
    "  * Looks at specific **conditional columns** (e.g., `drug_013`, `hba1c_result`).\n",
    "  * For each condition value (e.g., `hba1c_result = 'High'`), it **filters** the synthetic data to get matching records.\n",
    "  * Within that subset, it compares the **numeric features** (`lab_test_count`, `stay_duration_days`) of the synthetic records to the value in the test record.\n",
    "\n",
    "This is done for **each row** in the test set, so the synthetic dataset is repeatedly queried and filtered based on the conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Against what does it compare the synthetic data metrics, values, or parameters to make predictions?**\n",
    "\n",
    "The attack compares **each individual test row** against the **synthetic data distribution** that matches on the conditional feature.\n",
    "\n",
    "Let’s make this concrete:\n",
    "\n",
    "* Suppose a test row has `hba1c_result = \"High\"` and `lab_test_count = 4`.\n",
    "* The attack:\n",
    "\n",
    "  * Finds all synthetic rows where `hba1c_result == \"High\"`.\n",
    "  * Extracts all their `lab_test_count` values.\n",
    "  * Compares the test value (4) against the synthetic distribution using the **Kolmogorov–Smirnov (KS) statistic**, which measures the difference between distributions.\n",
    "\n",
    "This is repeated for:\n",
    "\n",
    "* Every conditional column (`drug_013`, `hba1c_result`)\n",
    "* Every numeric target column (`lab_test_count`, `stay_duration_days`)\n",
    "\n",
    "The KS statistic values are averaged to produce a **final score** for each test row. This score indicates how closely that test record resembles the synthetic distribution. A **low score** means the test row matches the synthetic distribution well (possible member), and a **high score** means it doesn’t match well (likely non-member).\n",
    "\n",
    "Predictions are made by thresholding these scores:\n",
    "\n",
    "* If the score is **lower than the median**, classify as member.\n",
    "* If it's **higher**, classify as non-member.\n",
    "\n",
    "The predicted labels are then evaluated against the true labels using:\n",
    "\n",
    "* Accuracy, Precision, Recall, F1 Score, AUC (ROC), and ASR (Attack Success Rate).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Extremely simple overview of the attack**\n",
    "\n",
    "This attack checks:\n",
    "\n",
    "> \"Does this test record *fit too well* with the synthetic data when we look at specific conditions and outcomes?\"\n",
    "\n",
    "If it fits the synthetic data *too closely* under certain conditions (e.g., \"patients who took drug\\_013 = 1 have stay\\_duration\\_days = 5\"), it’s probably a **member** — i.e., the synthetic model saw it during training.\n",
    "\n",
    "So for every test row, the attack:\n",
    "\n",
    "* Looks at conditional values like drug prescriptions or test results.\n",
    "* Filters the synthetic data by these conditions.\n",
    "* Compares the test value against that subset using a statistical test (KS-test).\n",
    "* Scores how much the test row stands out.\n",
    "* Guesses if it was in the training set (member) or not based on that score."
   ],
   "id": "e743a2610965b518"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T09:29:13.522190Z",
     "start_time": "2025-07-24T09:29:12.889915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# ─── Configuration ──────────────────────────────────────────────────────────────\n",
    "FOLDER_PATH = \"dataset/MIA_SDG_Exercise\"\n",
    "SYNTHETIC_FILES = [f\"synthetic_data{i}.csv\" for i in range(1, 5)]\n",
    "TEST_FILES = [\"test_data_with_outliers.csv\", \"test_data_wto_outliers.csv\"]\n",
    "LABEL_COLUMN = \"is_member\"\n",
    "\n",
    "CONDITION_COLUMNS = [\"drug_013\", \"hba1c_result\"]\n",
    "NUMERIC_TARGETS    = [\"lab_test_count\", \"stay_duration_days\"]\n",
    "\n",
    "# ─── Conditional Imbalance Attack ───────────────────────────────────────────────\n",
    "def compute_conditional_imbalance_scores(\n",
    "    synth_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each test row, if its condition value is rare in the test set,\n",
    "    compute a KS‐statistic between the synthetic values for that condition\n",
    "    and the single test value (repeated). Average across conditions & targets.\n",
    "    Returns an array of length len(test_df).\n",
    "    \"\"\"\n",
    "    # 1) Identify rare condition values from the test set\n",
    "    rare_values = {}\n",
    "    for col in CONDITION_COLUMNS:\n",
    "        if col in test_df.columns:\n",
    "            vc = test_df[col].value_counts(normalize=True)\n",
    "            rare_values[col] = vc[vc < 0.2].index.tolist()\n",
    "\n",
    "    # 2) Score each row\n",
    "    scores = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        total_stat = 0.0\n",
    "        count = 0\n",
    "        for cond_col in CONDITION_COLUMNS:\n",
    "            if cond_col not in row or cond_col not in synth_df.columns:\n",
    "                continue\n",
    "            val = row[cond_col]\n",
    "            if val not in rare_values.get(cond_col, []):\n",
    "                continue\n",
    "\n",
    "            # pull synthetic subset for this condition value\n",
    "            subset = synth_df[synth_df[cond_col] == val]\n",
    "            for target in NUMERIC_TARGETS:\n",
    "                if target in subset.columns and target in row.index:\n",
    "                    synth_vals = subset[target].dropna()\n",
    "                    test_val   = row[target]\n",
    "                    if pd.notna(test_val) and len(synth_vals) > 5:\n",
    "                        stat, _ = ks_2samp(synth_vals, [test_val] * len(synth_vals))\n",
    "                        total_stat += stat\n",
    "                        count += 1\n",
    "\n",
    "        # average over all computed stats, or zero if none applied\n",
    "        scores.append(total_stat / count if count > 0 else 0.0)\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def evaluate_attack(\n",
    "    scores: np.ndarray,\n",
    "    labels: pd.Series,\n",
    "    threshold: float = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute standard metrics given per‐row scores and true 'yes'/'no' labels.\n",
    "    If threshold is None, use the median score.\n",
    "    \"\"\"\n",
    "    y_scores = np.array(scores)\n",
    "    y_true   = np.array([1 if l == \"yes\" else 0 for l in labels])\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = np.median(y_scores)\n",
    "\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\" : accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\"   : recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\"       : f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC\"      : roc_auc_score(y_true, y_scores),\n",
    "        \"ASR\"      : float((y_pred == y_true).mean())\n",
    "    }\n",
    "\n",
    "\n",
    "def run_conditional_imbalance_attack() -> dict:\n",
    "    \"\"\"\n",
    "    Run the fixed Conditional Imbalance attack on all synth/test pairs.\n",
    "    Returns a mapping \"synth.csv vs test.csv\" -> metrics dict.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for synth_file in SYNTHETIC_FILES:\n",
    "        synth_path = os.path.join(FOLDER_PATH, synth_file)\n",
    "        synth_df   = pd.read_csv(synth_path)\n",
    "\n",
    "        for test_file in TEST_FILES:\n",
    "            test_path = os.path.join(FOLDER_PATH, test_file)\n",
    "            test_df   = pd.read_csv(test_path)\n",
    "\n",
    "            if LABEL_COLUMN not in test_df.columns:\n",
    "                print(f\"⚠️  Skipping {test_file}: no '{LABEL_COLUMN}' column.\")\n",
    "                continue\n",
    "\n",
    "            key = f\"{synth_file} vs {test_file}\"\n",
    "            print(f\"🔍 Running Conditional Imbalance Attack on: {key}\")\n",
    "\n",
    "            # Align columns (drop the label from features)\n",
    "            common_cols   = [c for c in test_df.columns\n",
    "                             if c in synth_df.columns and c != LABEL_COLUMN]\n",
    "            test_subset   = test_df[common_cols + [LABEL_COLUMN]].dropna(subset=[LABEL_COLUMN])\n",
    "            synth_subset  = synth_df[common_cols].copy()\n",
    "\n",
    "            labels = test_subset[LABEL_COLUMN]\n",
    "            X_test = test_subset.drop(columns=[LABEL_COLUMN])\n",
    "\n",
    "            # Compute per‐row conditional imbalance scores\n",
    "            scores = compute_conditional_imbalance_scores(synth_subset, X_test)\n",
    "\n",
    "            # Evaluate and store\n",
    "            results[key] = evaluate_attack(scores, labels)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results: dict):\n",
    "    \"\"\"Print the results for each dataset pair.\"\"\"\n",
    "    for pair, metrics in results.items():\n",
    "        print(f\"\\n==== Results for {pair} ====\")\n",
    "        for name, val in metrics.items():\n",
    "            print(f\"{name:>10}: {val:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    attack_results = run_conditional_imbalance_attack()\n",
    "    print_results(attack_results)"
   ],
   "id": "61be728c45d2679d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Conditional Imbalance Attack on: synthetic_data1.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Conditional Imbalance Attack on: synthetic_data1.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Conditional Imbalance Attack on: synthetic_data2.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Conditional Imbalance Attack on: synthetic_data2.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Conditional Imbalance Attack on: synthetic_data3.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Conditional Imbalance Attack on: synthetic_data3.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Conditional Imbalance Attack on: synthetic_data4.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Conditional Imbalance Attack on: synthetic_data4.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.6293\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.5400\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.6324\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.5550\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.6419\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.5400\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.6198\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.5375\n",
      "       ASR: 0.5000\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attack 3\n",
    "This script implements a **Gaussianity-based Membership Inference Attack (MIA)** on synthetic data. It tries to infer whether a real data record was used in training the synthetic data generator, based on how \"normal\" (in the statistical sense) the generated numeric feature distributions are.\n",
    "\n",
    "Let’s explain it thoroughly using your three requested points:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. How it uses the given synthetic data? How does it process it?**\n",
    "\n",
    "The script uses the synthetic datasets (`synthetic_data1.csv` to `synthetic_data4.csv`) as the **output** of a generative model trained on private (real) data. It assumes that these synthetic datasets **approximate the distributions** of the real training records.\n",
    "\n",
    "Here’s how it processes them:\n",
    "\n",
    "* Loads each synthetic dataset (`synth_df`) using `pandas.read_csv`.\n",
    "* Loads corresponding real-world test data (`test_df`), which contains a label `is_member` marking whether each record was used to train the generator.\n",
    "* Aligns columns between `synth_df` and `test_df`, removing the label from `synth_df`.\n",
    "* For **each row** in the test data:\n",
    "\n",
    "  * Drops the `is_member` label to isolate the features.\n",
    "  * Uses only the numeric columns.\n",
    "  * For each numeric column, it:\n",
    "\n",
    "    * Pulls the column’s values from the synthetic dataset (i.e., the distribution).\n",
    "    * Runs **normality tests** (Shapiro-Wilk and Anderson-Darling) on that distribution.\n",
    "    * Uses the test result statistics to compute a **\"Gaussianity score\"** — a numeric measure of how normally distributed the synthetic data looks in that column.\n",
    "  * Averages these scores across all numeric columns.\n",
    "\n",
    "This gives a **score per test row** reflecting how \"Gaussian\" the synthetic data looks for the features of that row.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Against what does it compare the synthetic data metrics, values, or parameters to make predictions?**\n",
    "\n",
    "The attack doesn’t compare one test row directly against other test rows. Instead, it compares **how well the synthetic data fits a normal distribution** for the numeric features present in the test row.\n",
    "\n",
    "Here’s the key logic:\n",
    "\n",
    "* For each test row, we compute a Gaussianity score based on **the synthetic distribution** of numeric features relevant to that row.\n",
    "* The **core idea**: If the synthetic generator overfit some training data, its generated values will **look less like Gaussian noise** and more like sharp memorized patterns. So the **less Gaussian** the synthetic feature distributions appear (especially in columns where the test row has a value), the more likely it is that the test row **was used during training**.\n",
    "* These scores are collected for all test rows and thresholded (median by default) to classify them as **\"yes\" (member)** or **\"no\" (non-member)**.\n",
    "\n",
    "Predictions are evaluated using:\n",
    "\n",
    "* Accuracy, Precision, Recall, F1 score, AUC, and ASR (Attack Success Rate).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Extremely simple overview of the attack**\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "> \"If the synthetic data is **not very noisy** and seems to follow **non-Gaussian patterns**, it probably reflects the real data it was trained on.\"\n",
    "\n",
    "So for each real test record:\n",
    "\n",
    "* The attacker looks at the synthetic values for each numeric column.\n",
    "* Measures how \"Gaussian\" (normally distributed) these synthetic values are.\n",
    "* If the synthetic values **deviate from a normal shape**, that could mean the generator memorized specific values.\n",
    "* If this happens in columns related to the test record, it may mean the record was **in the training set** (i.e., a **member**).\n",
    "\n",
    "The attack **doesn’t look at the test row’s actual values** beyond using them to decide which columns to analyze. Instead, it checks how \"overfit\" the generator appears, assuming training data would make the synthetic output **less Gaussian**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Bonus: Why this attack can work\n",
    "\n",
    "Generative models like CTGAN or Gaussian copulas often try to add randomness to mimic real data. But if they accidentally memorize the training data, the generated distributions become:\n",
    "\n",
    "* Too sharp\n",
    "* Too flat\n",
    "* Not noisy enough\n",
    "\n",
    "These flaws can be caught using statistical tests for **normality**, and that’s what this attack exploits.\n",
    "\n",
    "Let me know if you want to visualize how Shapiro and Anderson behave on synthetic distributions or want to combine this with other attacks like mode collapse or conditional imbalance.\n"
   ],
   "id": "d470e28917eb3b22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T09:31:45.602855Z",
     "start_time": "2025-07-24T09:31:45.313464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ─── Configuration ──────────────────────────────────────────────────────────────\n",
    "FOLDER_PATH     = \"dataset/MIA_SDG_Exercise\"\n",
    "SYNTHETIC_FILES = [f\"synthetic_data{i}.csv\" for i in range(1, 5)]\n",
    "TEST_FILES      = [\"test_data_with_outliers.csv\", \"test_data_wto_outliers.csv\"]\n",
    "LABEL_COLUMN    = \"is_member\"\n",
    "\n",
    "# Which numeric columns to use in the Gaussianity attack\n",
    "NUMERIC_COLUMNS = [\n",
    "    \"lab_test_count\",\n",
    "    \"medication_count\",\n",
    "    \"stay_duration_days\",\n",
    "    \"inpatient_visits\",\n",
    "    \"diagnosis_count\",\n",
    "    \"procedure_count\",\n",
    "    \"entry_type_code\",\n",
    "    \"entry_origin_code\",\n",
    "    \"outpatient_visits\",\n",
    "    \"emergency_visits\",\n",
    "    \"exit_status_code\"\n",
    "]\n",
    "\n",
    "# ─── Gaussianity‐Based Attack ────────────────────────────────────────────────────\n",
    "def compute_gaussianity_scores(\n",
    "    synth_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    numeric_cols: list\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each row in test_df, compute a per-row Gaussian‐deviation score:\n",
    "      1. Fit a normal distribution (µ, σ) on each numeric column in synth_df.\n",
    "      2. For each test-row value x_i, compute z = |x_i - µ|/σ.\n",
    "      3. Average those z-scores across numeric_cols.\n",
    "    Returns an array of length len(test_df).\n",
    "    \"\"\"\n",
    "    # 1) Precompute µ and σ on the synthetic distribution\n",
    "    mu    = synth_df[numeric_cols].mean()\n",
    "    sigma = synth_df[numeric_cols].std(ddof=0)  # population std\n",
    "    # Avoid division by zero\n",
    "    sigma = sigma.replace(0, np.finfo(float).eps)\n",
    "\n",
    "    # 2) Score each row\n",
    "    scores = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        z_scores = []\n",
    "        for col in numeric_cols:\n",
    "            if col in row and pd.notna(row[col]):\n",
    "                z = abs(row[col] - mu[col]) / sigma[col]\n",
    "                z_scores.append(z)\n",
    "        scores.append(np.mean(z_scores) if z_scores else 0.0)\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def evaluate_attack(\n",
    "    scores: np.ndarray,\n",
    "    labels: pd.Series,\n",
    "    threshold: float = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Given per-row scores and true 'yes'/'no' labels, compute standard metrics.\n",
    "    If threshold is None, use the median of the scores.\n",
    "    \"\"\"\n",
    "    y_scores = np.array(scores)\n",
    "    y_true   = np.array([1 if l == \"yes\" else 0 for l in labels])\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = np.median(y_scores)\n",
    "\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\" : accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\"   : recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\"       : f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC\"      : roc_auc_score(y_true, y_scores),\n",
    "        \"ASR\"      : float((y_pred == y_true).mean())\n",
    "    }\n",
    "\n",
    "\n",
    "def run_gaussianity_attack() -> dict:\n",
    "    \"\"\"\n",
    "    Run the fixed Gaussianity attack across all synthetic vs. test dataset pairs.\n",
    "    Returns a dict mapping \"synth.csv vs test.csv\" -> metrics dict.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for synth_file in SYNTHETIC_FILES:\n",
    "        synth_df = pd.read_csv(os.path.join(FOLDER_PATH, synth_file))\n",
    "\n",
    "        for test_file in TEST_FILES:\n",
    "            test_df = pd.read_csv(os.path.join(FOLDER_PATH, test_file))\n",
    "            if LABEL_COLUMN not in test_df.columns:\n",
    "                print(f\"⚠️ Skipping {test_file}: no '{LABEL_COLUMN}'\")\n",
    "                continue\n",
    "\n",
    "            key = f\"{synth_file} vs {test_file}\"\n",
    "            print(f\"🔍 Running Gaussianity Attack on: {key}\")\n",
    "\n",
    "            # Align columns (drop the label from features)\n",
    "            common_cols  = [\n",
    "                c for c in test_df.columns\n",
    "                if c in synth_df.columns and c != LABEL_COLUMN\n",
    "            ]\n",
    "            test_subset  = test_df[common_cols + [LABEL_COLUMN]].dropna(subset=[LABEL_COLUMN])\n",
    "            synth_subset = synth_df[common_cols].copy()\n",
    "\n",
    "            # Extract labels and feature-only DataFrame\n",
    "            labels = test_subset[LABEL_COLUMN]\n",
    "            X_test = test_subset.drop(columns=[LABEL_COLUMN])\n",
    "\n",
    "            # Determine which numeric columns are actually present\n",
    "            num_cols = [c for c in NUMERIC_COLUMNS if c in common_cols]\n",
    "\n",
    "            # Compute per-row Gaussianity scores\n",
    "            scores = compute_gaussianity_scores(synth_subset, X_test, num_cols)\n",
    "\n",
    "            # Evaluate and store\n",
    "            results[key] = evaluate_attack(scores, labels)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results: dict):\n",
    "    \"\"\"Nicely prints the attack metrics.\"\"\"\n",
    "    for pair, metrics in results.items():\n",
    "        print(f\"\\n==== Results for {pair} ====\")\n",
    "        for name, val in metrics.items():\n",
    "            print(f\"{name:>10}: {val:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    attack_results = run_gaussianity_attack()\n",
    "    print_results(attack_results)"
   ],
   "id": "3699e494d92693ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Gaussianity Attack on: synthetic_data1.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Gaussianity Attack on: synthetic_data1.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Gaussianity Attack on: synthetic_data2.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Gaussianity Attack on: synthetic_data2.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Gaussianity Attack on: synthetic_data3.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Gaussianity Attack on: synthetic_data3.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Gaussianity Attack on: synthetic_data4.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Gaussianity Attack on: synthetic_data4.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.9375\n",
      " Precision: 0.9375\n",
      "    Recall: 0.9375\n",
      "        F1: 0.9375\n",
      "       AUC: 0.9748\n",
      "       ASR: 0.9375\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5500\n",
      " Precision: 0.5500\n",
      "    Recall: 0.5500\n",
      "        F1: 0.5500\n",
      "       AUC: 0.5575\n",
      "       ASR: 0.5500\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.8125\n",
      " Precision: 0.8125\n",
      "    Recall: 0.8125\n",
      "        F1: 0.8125\n",
      "       AUC: 0.9084\n",
      "       ASR: 0.8125\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5500\n",
      " Precision: 0.5500\n",
      "    Recall: 0.5500\n",
      "        F1: 0.5500\n",
      "       AUC: 0.5975\n",
      "       ASR: 0.5500\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.9375\n",
      " Precision: 0.9375\n",
      "    Recall: 0.9375\n",
      "        F1: 0.9375\n",
      "       AUC: 0.9688\n",
      "       ASR: 0.9375\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.6000\n",
      " Precision: 0.6000\n",
      "    Recall: 0.6000\n",
      "        F1: 0.6000\n",
      "       AUC: 0.5800\n",
      "       ASR: 0.6000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.9167\n",
      " Precision: 0.9167\n",
      "    Recall: 0.9167\n",
      "        F1: 0.9167\n",
      "       AUC: 0.9644\n",
      "       ASR: 0.9167\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5500\n",
      " Precision: 0.5500\n",
      "    Recall: 0.5500\n",
      "        F1: 0.5500\n",
      "       AUC: 0.6025\n",
      "       ASR: 0.5500\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attack 4\n",
    "\n",
    "This attack exploits **clustering bias**: if a synthetic data generator has memorized or over-fit its training records, then including one of those records alongside the synthetic samples will **change the clustering structure** in a detectable way.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. How it uses the given synthetic data\n",
    "\n",
    "1. **Load synthetic datasets**  \n",
    "   Each `synthetic_dataN.csv` is read into a DataFrame `synth_df`.  \n",
    "2. **Select numeric features**  \n",
    "   A predefined list `NUMERIC_COLUMNS` (e.g. `lab_test_count`, `stay_duration_days`, etc.) is used.  \n",
    "3. **For each test row**  \n",
    "   - Drop the `\"is_member\"` label to isolate feature values.  \n",
    "   - Create a one-row DataFrame `row_df` containing just those numeric features.  \n",
    "4. **Combine**  \n",
    "   - Vertically concatenate the full `synth_df[NUMERIC_COLUMNS]` with `row_df` ⇒ `df_combined`.  \n",
    "   - This puts the single test record “in the same space” as all synthetic samples.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Scoring Mechanism: Clustering + Silhouette\n",
    "\n",
    "1. **Standardize**  \n",
    "   ```python\n",
    "   X = StandardScaler().fit_transform(df_combined.values)\n",
    "   ```\n",
    "\n",
    "Centers each feature to mean 0 and scales to unit variance, so clustering isn’t dominated by scale differences.\n",
    "\n",
    "2. **K-means clustering**\n",
    "\n",
    "   ```python\n",
    "   kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "   labels = kmeans.fit_predict(X)\n",
    "   ```\n",
    "\n",
    "   Partitions the combined data into 2 clusters. One cluster may “capture” the test record if it is too similar to memorized samples.\n",
    "\n",
    "3. **Silhouette score**\n",
    "\n",
    "   ```python\n",
    "   score = silhouette_score(X, labels)\n",
    "   ```\n",
    "\n",
    "   Measures **cohesion vs. separation**: high score → clusters are tight and well separated.\n",
    "\n",
    "   * If inserting a **member** record causes the synthetic cloud to split unnaturally, silhouette will change.\n",
    "   * If inserting a **non-member** record just sits in noise, clustering structure is less disrupted.\n",
    "\n",
    "4. **Per-row score**\n",
    "\n",
    "   * Repeat for each test record ⇒ array of silhouette scores, one per row.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Prediction & Evaluation\n",
    "\n",
    "1. **Thresholding**\n",
    "\n",
    "   * Default: median of all per-row silhouette scores\n",
    "   * Records with score ≥ threshold ⇒ predicted **member**\n",
    "   * Otherwise ⇒ **non-member**\n",
    "\n",
    "2. **Metrics**\n",
    "\n",
    "   * **Accuracy**, **Precision**, **Recall**, **F1**\n",
    "   * **AUC** (ROC area under curve using raw scores)\n",
    "   * **ASR** (Attack Success Rate = fraction of correct predictions)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Intuition & Limitations\n",
    "\n",
    "* **Intuition**\n",
    "\n",
    "  * A genuine training record, when added, will sit “inside” a high-density region of synthetic samples, potentially splitting that region into two clusters and **raising** the silhouette score.\n",
    "  * An out-of-training (non-member) record will behave like an outlier or sit near the edge, having **less impact** on cluster compactness.\n",
    "\n",
    "* **Limitations**\n",
    "\n",
    "  * **Choice of k=2** is heuristic—more clusters or a different clustering method might be needed for complex data.\n",
    "  * **Silhouette sensitivity**: depends on global data geometry; noise or uneven densities can reduce signal.\n",
    "  * **High dimensionality**: many numeric features may dilute clustering effects unless carefully standardized or dimensionality-reduced first.\n",
    "\n",
    "\n"
   ],
   "id": "cf4412776340cc2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T09:53:22.757059Z",
     "start_time": "2025-07-24T09:49:49.391057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "FOLDER_PATH = \"dataset/MIA_SDG_Exercise\"\n",
    "SYNTHETIC_FILES = [f\"synthetic_data{i}.csv\" for i in range(1, 5)]\n",
    "TEST_FILES = [\"test_data_with_outliers.csv\", \"test_data_wto_outliers.csv\"]\n",
    "LABEL_COLUMN = \"is_member\"\n",
    "\n",
    "NUMERIC_COLUMNS = [\n",
    "    \"lab_test_count\", \"medication_count\", \"stay_duration_days\", \"inpatient_visits\",\n",
    "    \"diagnosis_count\", \"procedure_count\", \"entry_type_code\", \"entry_origin_code\",\n",
    "    \"outpatient_visits\", \"emergency_visits\", \"exit_status_code\"\n",
    "]\n",
    "\n",
    "# --- Evaluation Helper --- #\n",
    "def evaluate_attack(scores, labels, threshold=None):\n",
    "    scores = np.array(scores)\n",
    "    true_labels = np.array([1 if l == \"yes\" else 0 for l in labels])\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = np.median(scores)\n",
    "    preds = (scores >= threshold).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(true_labels, preds),\n",
    "        \"Precision\": precision_score(true_labels, preds, zero_division=0),\n",
    "        \"Recall\": recall_score(true_labels, preds, zero_division=0),\n",
    "        \"F1\": f1_score(true_labels, preds, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(true_labels, scores),\n",
    "        \"ASR\": float(np.mean(preds == true_labels))\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# --- Main Score Function --- #\n",
    "def compute_clustering_bias_score(synth_df, test_row, cols, n_clusters=2):\n",
    "    # Combine test_row + synthetic sample space\n",
    "    row_df = pd.DataFrame([test_row[cols]])\n",
    "    df_combined = pd.concat([synth_df[cols], row_df], ignore_index=True).dropna()\n",
    "\n",
    "    if df_combined.shape[0] < n_clusters + 1:\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        X = StandardScaler().fit_transform(df_combined.values)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "        score = silhouette_score(X, cluster_labels)\n",
    "        return score\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# --- Attack Execution --- #\n",
    "def run_reconstruction_bias_attack():\n",
    "    results = defaultdict(dict)\n",
    "\n",
    "    for synth_file in SYNTHETIC_FILES:\n",
    "        synth_path = os.path.join(FOLDER_PATH, synth_file)\n",
    "        synth_df = pd.read_csv(synth_path)\n",
    "\n",
    "        for test_file in TEST_FILES:\n",
    "            test_path = os.path.join(FOLDER_PATH, test_file)\n",
    "            test_df = pd.read_csv(test_path)\n",
    "\n",
    "            key = f\"{synth_file} vs {test_file}\"\n",
    "            print(f\"\\n🔍 Running Reconstruction Bias Attack on: {key}\")\n",
    "\n",
    "            common_cols = [col for col in test_df.columns if col in synth_df.columns and col != LABEL_COLUMN]\n",
    "            test_subset = test_df[common_cols + [LABEL_COLUMN]].dropna(subset=[LABEL_COLUMN])\n",
    "            synth_subset = synth_df[common_cols].copy()\n",
    "\n",
    "            all_labels = test_subset[LABEL_COLUMN].tolist()\n",
    "            scores = []\n",
    "\n",
    "            for _, row in test_subset.iterrows():\n",
    "                score = compute_clustering_bias_score(synth_subset, row, NUMERIC_COLUMNS, n_clusters=2)\n",
    "                scores.append(score)\n",
    "\n",
    "            metrics = evaluate_attack(scores, all_labels)\n",
    "            results[key][\"Reconstruction Bias\"] = metrics\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Print Results --- #\n",
    "def print_results(results):\n",
    "    for pair, attacks in results.items():\n",
    "        print(f\"\\n==== Results for {pair} ====\")\n",
    "        for attack_name, metrics in attacks.items():\n",
    "            print(f\"Attack: {attack_name}\")\n",
    "            for metric, val in metrics.items():\n",
    "                print(f\"{metric:>10}: {val:.4f}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    attack_results = run_reconstruction_bias_attack()\n",
    "    print_results(attack_results)\n"
   ],
   "id": "d49e09119da173b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running Reconstruction Bias Attack on: synthetic_data1.csv vs test_data_with_outliers.csv\n",
      "\n",
      "🔍 Running Reconstruction Bias Attack on: synthetic_data1.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "🔍 Running Reconstruction Bias Attack on: synthetic_data2.csv vs test_data_with_outliers.csv\n",
      "\n",
      "🔍 Running Reconstruction Bias Attack on: synthetic_data2.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "🔍 Running Reconstruction Bias Attack on: synthetic_data3.csv vs test_data_with_outliers.csv\n",
      "\n",
      "🔍 Running Reconstruction Bias Attack on: synthetic_data3.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "🔍 Running Reconstruction Bias Attack on: synthetic_data4.csv vs test_data_with_outliers.csv\n",
      "\n",
      "🔍 Running Reconstruction Bias Attack on: synthetic_data4.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_with_outliers.csv ====\n",
      "Attack: Reconstruction Bias\n",
      "  Accuracy: 0.8542\n",
      " Precision: 0.8542\n",
      "    Recall: 0.8542\n",
      "        F1: 0.8542\n",
      "       AUC: 0.8490\n",
      "       ASR: 0.8542\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_wto_outliers.csv ====\n",
      "Attack: Reconstruction Bias\n",
      "  Accuracy: 0.5500\n",
      " Precision: 0.5500\n",
      "    Recall: 0.5500\n",
      "        F1: 0.5500\n",
      "       AUC: 0.5625\n",
      "       ASR: 0.5500\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_with_outliers.csv ====\n",
      "Attack: Reconstruction Bias\n",
      "  Accuracy: 0.0625\n",
      " Precision: 0.0625\n",
      "    Recall: 0.0625\n",
      "        F1: 0.0625\n",
      "       AUC: 0.0365\n",
      "       ASR: 0.0625\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_wto_outliers.csv ====\n",
      "Attack: Reconstruction Bias\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 0.5000\n",
      "        F1: 0.5000\n",
      "       AUC: 0.4950\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_with_outliers.csv ====\n",
      "Attack: Reconstruction Bias\n",
      "  Accuracy: 0.7083\n",
      " Precision: 0.7083\n",
      "    Recall: 0.7083\n",
      "        F1: 0.7083\n",
      "       AUC: 0.7617\n",
      "       ASR: 0.7083\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_wto_outliers.csv ====\n",
      "Attack: Reconstruction Bias\n",
      "  Accuracy: 0.6000\n",
      " Precision: 0.6000\n",
      "    Recall: 0.6000\n",
      "        F1: 0.6000\n",
      "       AUC: 0.6025\n",
      "       ASR: 0.6000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_with_outliers.csv ====\n",
      "Attack: Reconstruction Bias\n",
      "  Accuracy: 0.2500\n",
      " Precision: 0.2500\n",
      "    Recall: 0.2500\n",
      "        F1: 0.2500\n",
      "       AUC: 0.2261\n",
      "       ASR: 0.2500\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_wto_outliers.csv ====\n",
      "Attack: Reconstruction Bias\n",
      "  Accuracy: 0.4500\n",
      " Precision: 0.4500\n",
      "    Recall: 0.4500\n",
      "        F1: 0.4500\n",
      "       AUC: 0.4425\n",
      "       ASR: 0.4500\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attack 5\n",
    "\n",
    "This attack exploits how a synthetic data generator may inadvertently **preserve or distort feature dependencies** from its training data. By measuring, for each test record, how much the relationship between a categorical feature and a numeric target in the synthetic data deviates from that same relationship in the real (test) data, we can infer whether that record was likely used in training.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Goal\n",
    "\n",
    "- **Membership Inference**: Decide, for each test record, whether it was part of the original training set that produced the synthetic data.\n",
    "- **Key Idea**: If the generator overfit, it will reproduce certain category–numeric dependencies very closely for training members, while generalizing differently for non-members.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Inputs\n",
    "\n",
    "- **Synthetic datasets**: Four files (`synthetic_data1.csv` … `synthetic_data4.csv`), each produced by a different private-data synthesizer.\n",
    "- **Test datasets**: Two files:\n",
    "  - `test_data_with_outliers.csv` (outlier‐filtered samples)\n",
    "  - `test_data_wto_outliers.csv` (random samples)\n",
    "- Each test row has a ground-truth label in column `is_member` (`\"yes\"`/`\"no\"`).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Feature Pairs\n",
    "\n",
    "We focus on a small set of categorical–numeric pairs believed to carry informative dependencies:\n",
    "\n",
    "| Categorical Feature    | Numeric Target      |\n",
    "|------------------------|---------------------|\n",
    "| `primary_diagnosis`    | `procedure_count`   |\n",
    "| `secondary_diagnosis`  | `exit_status_code`  |\n",
    "| `age_range`            | `medication_count`  |\n",
    "| `ethnic_group`         | `inpatient_visits`  |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Attack Workflow\n",
    "\n",
    "### 4.1 Precompute Group Statistics\n",
    "\n",
    "For each `(cat, num)` pair:\n",
    "\n",
    "1. **On the synthetic data**:\n",
    "   - Group by the categorical column.\n",
    "   - Compute the **mean** and **standard deviation** of the numeric target within each category.\n",
    "2. **On the test data**:\n",
    "   - Do the same grouping and statistics.\n",
    "\n",
    "We store two dictionaries:\n",
    "- `synth_stats[(cat,num)] = (means_synth, stds_synth)`\n",
    "- `test_stats[(cat,num)]  = (means_test,  stds_test)`\n",
    "\n",
    "### 4.2 Per-Row Score Computation\n",
    "\n",
    "For each test record _r_:\n",
    "\n",
    "1. Initialize `total_diff = 0`, `count = 0`.\n",
    "2. For each `(cat, num)` pair where:\n",
    "   - Both `cat` and `num` exist in the data.\n",
    "   - The record’s value `c = r[cat]` and `y = r[num]` are non-missing.\n",
    "   - We have precomputed stats for category `c` in both synthetic and test data.\n",
    "3. Compute two standardized deviations:\n",
    "   - \\( z_\\mathrm{synth} = \\bigl|y - \\mu^\\mathrm{synth}_{c}\\bigr| / \\sigma^\\mathrm{synth}_{c} \\)\n",
    "   - \\( z_\\mathrm{test}  = \\bigl|y - \\mu^\\mathrm{test}_{c}\\bigr|  / \\sigma^\\mathrm{test}_{c} \\)\n",
    "4. Add the absolute difference \\( \\lvert z_\\mathrm{synth} - z_\\mathrm{test}\\rvert \\) to `total_diff`, increment `count`.\n",
    "5. Final per‐row score =\n",
    "   \\[\n",
    "     \\text{score}(r) \\;=\\; \n",
    "     \\begin{cases}\n",
    "       \\dfrac{\\text{total_diff}}{\\text{count}}, & \\text{if count}>0,\\\\\n",
    "       0, & \\text{otherwise.}\n",
    "     \\end{cases}\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Decision Rule\n",
    "\n",
    "- Collect all per-row scores into an array `scores`.\n",
    "- Choose a threshold \\( T \\) (default: **median** of `scores`).\n",
    "- Predict **member** if `score ≥ T`, else **non-member**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Evaluation Metrics\n",
    "\n",
    "For each synthetic vs. test pair, we compute:\n",
    "\n",
    "- **Accuracy**: fraction of correct predictions.\n",
    "- **Precision**: TP / (TP + FP).\n",
    "- **Recall**: TP / (TP + FN).\n",
    "- **F1-score**: harmonic mean of Precision & Recall.\n",
    "- **AUC**: area under the ROC curve, using the continuous `scores`.\n",
    "- **ASR** (Attack Success Rate): same as accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Intuition & Rationale\n",
    "\n",
    "- **Overfitting signature**: A generator that memorizes training records tends to reproduce their numeric values exactly (or very close) given their categorical context, leading to **smaller deviations** in the synthetic distribution than in held-out data.\n",
    "- **Relative comparison**: By contrasting how “far” a value lies from its category mean in both synthetic and test data (via z-scores), we capture whether a record fits *too well* the synthetic model (likely a member) or not (likely a non-member).\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Strengths & Limitations\n",
    "\n",
    "| Strengths                                         | Limitations                                            |\n",
    "|---------------------------------------------------|--------------------------------------------------------|\n",
    "| ✔️  Leverages simple, interpretable statistics     | ⚠️  Requires each category to appear often enough      |\n",
    "| ✔️  Doesn’t need complex model training            | ⚠️  Sensitive to noisy or low-variance categories      |\n",
    "| ✔️  Easily extends to more feature pairs           | ⚠️  May underperform if synthetic generator smooths out dependencies excessively |\n",
    "\n",
    "---\n",
    "\n",
    "> **Next steps**:  \n",
    "> - Experiment with different `(cat, num)` pairs or continuous–continuous dependency measures (e.g. Spearman correlation).  \n",
    "> - Blend this attack with marginal‐divergence or clustering‐based attacks for a more robust ensemble.  \n",
    "> - Tune the threshold \\( T \\) (e.g. optimize F1 on a validation split) instead of using the raw median.  \n"
   ],
   "id": "2dc8cd639db3b0ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T10:06:29.179751Z",
     "start_time": "2025-07-24T10:06:28.900566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ─── Configuration ──────────────────────────────────────────────────────────────\n",
    "FOLDER_PATH     = \"dataset/MIA_SDG_Exercise\"\n",
    "SYNTHETIC_FILES = [f\"synthetic_data{i}.csv\" for i in range(1, 5)]\n",
    "TEST_FILES      = [\"test_data_with_outliers.csv\", \"test_data_wto_outliers.csv\"]\n",
    "LABEL_COLUMN    = \"is_member\"\n",
    "\n",
    "# Pairs of (categorical, numeric) features for dependency-based scoring\n",
    "DEPENDENT_PAIRS = [\n",
    "    (\"primary_diagnosis\",   \"procedure_count\"),\n",
    "    (\"secondary_diagnosis\", \"exit_status_code\"),\n",
    "    (\"age_range\",           \"medication_count\"),\n",
    "    (\"ethnic_group\",        \"inpatient_visits\"),\n",
    "]\n",
    "\n",
    "# ─── Core Functions ─────────────────────────────────────────────────────────────\n",
    "\n",
    "def compute_dependency_scores(\n",
    "    synth_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each (cat, num) pair, compute per-category mean/std on both synth and test.\n",
    "    Then for each test row, for each pair:\n",
    "      z_synth = |y - μ_synth[x]| / σ_synth[x]\n",
    "      z_test  = |y - μ_test[x]|  / σ_test[x]\n",
    "      score += |z_synth - z_test|\n",
    "    Final per-row score = average over all valid pairs.\n",
    "    \"\"\"\n",
    "    # 1) Precompute group stats\n",
    "    synth_stats = {}\n",
    "    test_stats  = {}\n",
    "    eps = np.finfo(float).eps\n",
    "\n",
    "    for cat, num in DEPENDENT_PAIRS:\n",
    "        if cat in synth_df.columns and num in synth_df.columns:\n",
    "            grp = synth_df.groupby(cat)[num]\n",
    "            means = grp.mean().to_dict()\n",
    "            stds  = grp.std(ddof=0).replace(0, eps).to_dict()\n",
    "            synth_stats[(cat, num)] = (means, stds)\n",
    "\n",
    "        if cat in test_df.columns and num in test_df.columns:\n",
    "            grp = test_df.groupby(cat)[num]\n",
    "            means = grp.mean().to_dict()\n",
    "            stds  = grp.std(ddof=0).replace(0, eps).to_dict()\n",
    "            test_stats[(cat, num)] = (means, stds)\n",
    "\n",
    "    # 2) Score each test row\n",
    "    scores = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        total, count = 0.0, 0\n",
    "        for cat, num in DEPENDENT_PAIRS:\n",
    "            key = (cat, num)\n",
    "            if key not in synth_stats or key not in test_stats:\n",
    "                continue\n",
    "            if pd.isna(row.get(cat)) or pd.isna(row.get(num)):\n",
    "                continue\n",
    "\n",
    "            x_val = row[cat]\n",
    "            y_val = row[num]\n",
    "\n",
    "            synth_means, synth_stds = synth_stats[key]\n",
    "            test_means,  test_stds  = test_stats[key]\n",
    "\n",
    "            if x_val not in synth_means or x_val not in test_means:\n",
    "                continue\n",
    "\n",
    "            z_s = abs(y_val - synth_means[x_val]) / synth_stds[x_val]\n",
    "            z_t = abs(y_val - test_means[x_val])  / test_stds[x_val]\n",
    "            total += abs(z_s - z_t)\n",
    "            count += 1\n",
    "\n",
    "        scores.append(total / count if count > 0 else 0.0)\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def evaluate_attack(\n",
    "    scores: np.ndarray,\n",
    "    labels: pd.Series,\n",
    "    threshold: float = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute Accuracy, Precision, Recall, F1, AUC, and ASR given per-row scores\n",
    "    and true 'yes'/'no' labels. Uses median threshold if none provided.\n",
    "    \"\"\"\n",
    "    y_scores = np.array(scores)\n",
    "    y_true   = np.array([1 if l == \"yes\" else 0 for l in labels])\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = np.median(y_scores)\n",
    "\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\" : accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\"   : recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\"       : f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC\"      : roc_auc_score(y_true, y_scores),\n",
    "        \"ASR\"      : float((y_pred == y_true).mean())\n",
    "    }\n",
    "\n",
    "\n",
    "def run_dependency_attack() -> dict:\n",
    "    \"\"\"\n",
    "    Runs the fixed Overfitting Dependency attack on all synthetic/test pairs.\n",
    "    Returns a dict mapping \"synth.csv vs test.csv\" -> metrics dict.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for synth_file in SYNTHETIC_FILES:\n",
    "        synth_path = os.path.join(FOLDER_PATH, synth_file)\n",
    "        synth_df   = pd.read_csv(synth_path)\n",
    "\n",
    "        for test_file in TEST_FILES:\n",
    "            test_path = os.path.join(FOLDER_PATH, test_file)\n",
    "            test_df   = pd.read_csv(test_path)\n",
    "\n",
    "            if LABEL_COLUMN not in test_df.columns:\n",
    "                print(f\"⚠️  Skipping {test_file}: no '{LABEL_COLUMN}' column.\")\n",
    "                continue\n",
    "\n",
    "            key = f\"{synth_file} vs {test_file}\"\n",
    "            print(f\"🔍 Running Overfitting Dependency Attack on: {key}\")\n",
    "\n",
    "            # Align columns & split off labels\n",
    "            common_cols   = [\n",
    "                c for c in test_df.columns\n",
    "                if c in synth_df.columns and c != LABEL_COLUMN\n",
    "            ]\n",
    "            subset_test   = test_df[common_cols + [LABEL_COLUMN]].dropna(subset=[LABEL_COLUMN])\n",
    "            subset_synth  = synth_df[common_cols].copy()\n",
    "\n",
    "            labels = subset_test[LABEL_COLUMN]\n",
    "            X_test = subset_test.drop(columns=[LABEL_COLUMN])\n",
    "\n",
    "            # Compute per-row dependency scores\n",
    "            scores = compute_dependency_scores(subset_synth, X_test)\n",
    "\n",
    "            # Evaluate and store\n",
    "            results[key] = evaluate_attack(scores, labels)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results: dict):\n",
    "    \"\"\"Prints the metrics for each dataset pair.\"\"\"\n",
    "    for pair, metrics in results.items():\n",
    "        print(f\"\\n==== Results for {pair} ====\")\n",
    "        for name, val in metrics.items():\n",
    "            print(f\"{name:>10}: {val:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    attack_results = run_dependency_attack()\n",
    "    print_results(attack_results)"
   ],
   "id": "185e182e0f6dc7ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Overfitting Dependency Attack on: synthetic_data1.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Overfitting Dependency Attack on: synthetic_data1.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Overfitting Dependency Attack on: synthetic_data2.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Overfitting Dependency Attack on: synthetic_data2.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Overfitting Dependency Attack on: synthetic_data3.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Overfitting Dependency Attack on: synthetic_data3.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Overfitting Dependency Attack on: synthetic_data4.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Overfitting Dependency Attack on: synthetic_data4.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.8750\n",
      " Precision: 0.8750\n",
      "    Recall: 0.8750\n",
      "        F1: 0.8750\n",
      "       AUC: 0.9423\n",
      "       ASR: 0.8750\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 0.5000\n",
      "        F1: 0.5000\n",
      "       AUC: 0.4675\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.6667\n",
      " Precision: 0.6667\n",
      "    Recall: 0.6667\n",
      "        F1: 0.6667\n",
      "       AUC: 0.7348\n",
      "       ASR: 0.6667\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 0.5000\n",
      "        F1: 0.5000\n",
      "       AUC: 0.4025\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.6875\n",
      " Precision: 0.6875\n",
      "    Recall: 0.6875\n",
      "        F1: 0.6875\n",
      "       AUC: 0.7146\n",
      "       ASR: 0.6875\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 0.5000\n",
      "        F1: 0.5000\n",
      "       AUC: 0.4387\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.7917\n",
      " Precision: 0.7917\n",
      "    Recall: 0.7917\n",
      "        F1: 0.7917\n",
      "       AUC: 0.8438\n",
      "       ASR: 0.7917\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 0.5000\n",
      "        F1: 0.5000\n",
      "       AUC: 0.5975\n",
      "       ASR: 0.5000\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attack 6\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The Rare-Combo attack is a targeted **membership inference** technique designed to detect whether individual records from a real dataset were used to train a generative model. It exploits the presence (or absence) of **specific rare combinations** of feature values—“combos”—that occur infrequently in the real data. If a synthetic dataset reproduces these rare combos too faithfully, it suggests that those records were memorized by the generator.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Attack Motivation\n",
    "\n",
    "- **Generative models** sometimes overfit to rare, idiosyncratic slices of their training data.  \n",
    "- Rare feature combinations (e.g. a specific drug change flag paired with a particular provider specialty) are unlikely to be learned unless the exact record was included.  \n",
    "- By checking for these rare combos, we can derive a per-record score reflecting how “surprising” each test sample is, relative to the synthetic output.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data & Assumptions\n",
    "\n",
    "1. **Synthetic datasets**: Four CSV files (`synthetic_data1.csv` … `synthetic_data4.csv`), each generated by a different synthesizer (CTGAN, VAE, Bayesian network, Gaussian copula).  \n",
    "2. **Test datasets**: Two CSVs:\n",
    "   - `test_data_with_outliers.csv` (contains outlier records labeled `is_member=yes/no`)  \n",
    "   - `test_data_wto_outliers.csv` (random sample)  \n",
    "3. **Ground-truth label**: Each test row has an `is_member` column (“yes” if it was used during training, “no” otherwise).  \n",
    "4. **Feature space**: We assume all columns in `RARE_COMBOS` exist in both synthetic and test tables (any combos referencing missing columns are dropped).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Defining Rare Combos\n",
    "\n",
    "We predefine a small set of rare feature‐value combinations believed to be highly specific:\n",
    "\n",
    "| Combo # | Feature 1           | Feature 2    | Feature 3                   |\n",
    "|:-------:|:--------------------|:------------:|:---------------------------:|\n",
    "| 1       | `med_change_flag=decrease` | `drug_008=constant`   | `exit_status_code=9`          |\n",
    "| 2       | `drug_015=increase`       | `drug_006=constant`   | `inpatient_visits=4`          |\n",
    "| 3       | `drug_017=decrease`       | `ethnic_group=Asian`  | `provider_specialty=Orthopedic Surgeon` |\n",
    "\n",
    "> **Why these combos?**  \n",
    "> They involve rare drug‐change flags, demographic slices, and outcome codes that appear only in a handful of records. Their joint occurrence is extremely unlikely unless the model memorized those exact rows.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Scoring Methodology\n",
    "\n",
    "For each synthetic/test dataset pair:\n",
    "\n",
    "1. **Precompute synthetic frequencies**  \n",
    "   For each valid combo:\n",
    "   ```python\n",
    "   synth_freq = (# of rows in synth_df matching all combo conditions) \\\n",
    "                / (total # of rows in synth_df)\n"
   ],
   "id": "318636365f8e6606"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T10:20:21.191034Z",
     "start_time": "2025-07-24T10:20:20.109683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# ─── Configuration ──────────────────────────────────────────────────────────────\n",
    "FOLDER_PATH     = \"dataset/MIA_SDG_Exercise\"\n",
    "SYNTHETIC_FILES = [f\"synthetic_data{i}.csv\" for i in range(1, 5)]\n",
    "TEST_FILES      = [\"test_data_with_outliers.csv\", \"test_data_wto_outliers.csv\"]\n",
    "LABEL_COLUMN    = \"is_member\"\n",
    "\n",
    "RARE_COMBOS = [\n",
    "    {\"med_change_flag\": \"decrease\", \"drug_008\": \"constant\", \"exit_status_code\": 9},\n",
    "    {\"drug_015\": \"increase\",  \"drug_006\": \"constant\", \"inpatient_visits\": 4},\n",
    "    {\"drug_017\": \"decrease\",  \"ethnic_group\": \"Asian\", \"provider_specialty\": \"Orthopedic Surgeon\"},\n",
    "]\n",
    "\n",
    "# ─── Helper Functions ────────────────────────────────────────────────────────────\n",
    "def match_rare_combo(row: pd.Series, combo: dict) -> bool:\n",
    "    \"\"\"Return True if the row exactly matches the given combo dict.\"\"\"\n",
    "    for col, val in combo.items():\n",
    "        if col not in row or pd.isna(row[col]) or row[col] != val:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def compute_combo_frequency(df: pd.DataFrame, combo: dict) -> float:\n",
    "    \"\"\"\n",
    "    Compute the fraction of rows in df that match combo.\n",
    "    \"\"\"\n",
    "    matches = df.apply(lambda r: match_rare_combo(r, combo), axis=1)\n",
    "    return float(matches.mean())\n",
    "\n",
    "def compute_rare_combo_scores(\n",
    "    synth_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    combos: list[dict]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each test row and each valid combo:\n",
    "      - precompute synth_freq = fraction of synth_df matching combo\n",
    "      - indicator = 1 if row matches combo, else 0\n",
    "      - score_row += |synth_freq - indicator|\n",
    "    Final score per row = average over combos.\n",
    "    \"\"\"\n",
    "    # Precompute synth_freqs only for combos whose keys exist in synth_df\n",
    "    valid_combos = [\n",
    "        combo for combo in combos\n",
    "        if set(combo.keys()).issubset(synth_df.columns)\n",
    "    ]\n",
    "    synth_freqs = [\n",
    "        compute_combo_frequency(synth_df, combo)\n",
    "        for combo in valid_combos\n",
    "    ]\n",
    "    n = len(valid_combos)\n",
    "    if n == 0:\n",
    "        # No valid combos ⇒ zero scores\n",
    "        return np.zeros(len(test_df), dtype=float)\n",
    "\n",
    "    # Score each test row\n",
    "    scores = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        s = 0.0\n",
    "        for combo, freq in zip(valid_combos, synth_freqs):\n",
    "            indicator = 1.0 if match_rare_combo(row, combo) else 0.0\n",
    "            s += abs(freq - indicator)\n",
    "        scores.append(s / n)\n",
    "    return np.array(scores)\n",
    "\n",
    "def evaluate_attack(\n",
    "    scores: np.ndarray,\n",
    "    labels: pd.Series,\n",
    "    threshold: float = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute metrics given per-row scores and 'yes'/'no' labels.\n",
    "    If threshold is None, use median(scores).\n",
    "    \"\"\"\n",
    "    y_scores = scores\n",
    "    y_true   = np.array([1 if l == \"yes\" else 0 for l in labels])\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = np.median(y_scores)\n",
    "\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "    return {\n",
    "        \"Accuracy\" : accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\"   : recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\"       : f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC\"      : roc_auc_score(y_true, y_scores),\n",
    "        \"ASR\"      : float((y_pred == y_true).mean())\n",
    "    }\n",
    "\n",
    "# ─── Main Attack Runner ─────────────────────────────────────────────────────────\n",
    "def run_rare_combo_attacks() -> dict:\n",
    "    \"\"\"\n",
    "    Run the fixed Rare-Combo attack on all synthetic/test pairs.\n",
    "    Returns a dict mapping \"synth.csv vs test.csv\" -> metrics dict.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for synth_file in SYNTHETIC_FILES:\n",
    "        synth_df = pd.read_csv(os.path.join(FOLDER_PATH, synth_file))\n",
    "\n",
    "        for test_file in TEST_FILES:\n",
    "            test_df = pd.read_csv(os.path.join(FOLDER_PATH, test_file))\n",
    "            if LABEL_COLUMN not in test_df.columns:\n",
    "                print(f\"⚠️  Skipping {test_file}: no '{LABEL_COLUMN}' column.\")\n",
    "                continue\n",
    "\n",
    "            key = f\"{synth_file} vs {test_file}\"\n",
    "            print(f\"🔍 Running Rare Combo Attack on: {key}\")\n",
    "\n",
    "            # Align columns (drop label)\n",
    "            common_cols   = [\n",
    "                c for c in test_df.columns\n",
    "                if c in synth_df.columns and c != LABEL_COLUMN\n",
    "            ]\n",
    "            subset       = test_df[common_cols + [LABEL_COLUMN]].dropna(subset=[LABEL_COLUMN])\n",
    "            labels       = subset[LABEL_COLUMN]\n",
    "            X_test       = subset.drop(columns=[LABEL_COLUMN])\n",
    "            synth_subset = synth_df[common_cols].copy()\n",
    "\n",
    "            # Compute per-row scores\n",
    "            scores = compute_rare_combo_scores(synth_subset, X_test, RARE_COMBOS)\n",
    "\n",
    "            # Evaluate and store\n",
    "            results[key] = evaluate_attack(scores, labels)\n",
    "\n",
    "    return results\n",
    "\n",
    "def print_results(results: dict):\n",
    "    \"\"\"Nicely print the attack metrics.\"\"\"\n",
    "    for pair, metrics in results.items():\n",
    "        print(f\"\\n==== Results for {pair} ====\")\n",
    "        for name, val in metrics.items():\n",
    "            print(f\"{name:>10}: {val:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    attack_results = run_rare_combo_attacks()\n",
    "    print_results(attack_results)"
   ],
   "id": "55ab1f48d857a1d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Rare Combo Attack on: synthetic_data1.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Rare Combo Attack on: synthetic_data1.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Rare Combo Attack on: synthetic_data2.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Rare Combo Attack on: synthetic_data2.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Rare Combo Attack on: synthetic_data3.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Rare Combo Attack on: synthetic_data3.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Rare Combo Attack on: synthetic_data4.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Rare Combo Attack on: synthetic_data4.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.5000\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.5000\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.5000\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.5000\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.5000\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.5000\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.5000\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 1.0000\n",
      "        F1: 0.6667\n",
      "       AUC: 0.5000\n",
      "       ASR: 0.5000\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Attack 7: Copula-Based Membership Inference Attack\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "This attack exploits differences in both **marginal** and **joint** empirical distributions between a synthetic dataset and a held-out test set to decide whether individual records were used during the generative model’s training.\n",
    "\n",
    "- **Goal:** For each test record \\(x\\), compute a numerical score that reflects how “typical” \\(x\\) is under the synthetic data’s distribution.  \n",
    "- **Hypothesis:** Records seen during training produce synthetic samples whose joint behavior (across features) deviates less from the real test distribution, whereas unseen records induce larger discrepancies.\n",
    "\n",
    "## 2. Intuition\n",
    "\n",
    "1. **Marginal Discrepancy**  \n",
    "   For each numeric feature \\(f\\), compare the fraction of synthetic samples \\(\\{s_i\\}\\) with \\(s_i[f] \\le x[f]\\) to the fraction of test samples with the same property. A large difference suggests the model distorted that feature’s distribution—potentially by over-fitting to training members.\n",
    "\n",
    "2. **Joint Discrepancy**  \n",
    "   For selected feature pairs \\((f, g)\\), compare\n",
    "   \\[\n",
    "     \\Pr_{\\text{synth}}\\bigl(s[f] \\le x[f],\\,s[g]\\le x[g]\\bigr)\n",
    "     \\quad\\text{vs.}\\quad\n",
    "     \\Pr_{\\text{test}}\\bigl(t[f] \\le x[f],\\,t[g]\\le x[g]\\bigr).\n",
    "   \\]\n",
    "   Joint CDF differences capture dependencies (copula structure) that marginals alone miss.\n",
    "\n",
    "3. **Combined Score**  \n",
    "   \\[\n",
    "   \\text{score}(x)\n",
    "   = \\underbrace{\\frac{1}{|\\mathcal{J}|}\\sum_{(f,g)\\in\\mathcal{J}}\n",
    "        \\bigl|\\Pr_{\\text{synth}}(s[f]\\le x[f],s[g]\\le x[g])\n",
    "               -\\Pr_{\\text{test}}(t[f]\\le x[f],t[g]\\le x[g])\\bigr|}\n",
    "     }_{\\text{joint score}}\n",
    "   \\;-\\;\n",
    "   \\underbrace{\\frac{1}{|\\mathcal{M}|}\\sum_{f\\in\\mathcal{M}}\n",
    "        \\bigl|\\Pr_{\\text{synth}}(s[f]\\le x[f])\n",
    "               -\\Pr_{\\text{test}}(t[f]\\le x[f])\\bigr|\n",
    "     }_{\\text{marginal score}}\n",
    "   \\]\n",
    "   where \\(\\mathcal{M}\\) is the set of marginal columns and \\(\\mathcal{J}\\) the set of joint pairs.\n",
    "\n",
    "## 3. Methodology\n",
    "\n",
    "### 3.1 Data Alignment  \n",
    "- Load synthetic file and test file.  \n",
    "- Drop any rows without the ground-truth membership label (`is_member`).  \n",
    "- Identify **common columns** present in both datasets, excluding `is_member`.\n",
    "\n",
    "### 3.2 Marginal CDF Comparison  \n",
    "For each numeric column \\(f\\in\\text{MARGINAL\\_COLUMNS}\\):\n",
    "1. Let \\(x =\\) test record’s value in column \\(f\\).  \n",
    "2. Compute  \n",
    "   \\[\n",
    "     P_{\\text{synth}}(f \\le x) = \\frac{1}{N_{\\text{synth}}}\n",
    "       \\sum_{s\\in\\text{synth}} \\mathbb{1}\\{s[f] \\le x\\},\n",
    "   \\]\n",
    "   similarly for \\(P_{\\text{test}}(f \\le x)\\).  \n",
    "3. Record \\(|P_{\\text{synth}} - P_{\\text{test}}|\\).  \n",
    "4. Average these differences to get the **marginal score**.\n",
    "\n",
    "### 3.3 Joint CDF Comparison  \n",
    "For each feature-pair \\((f,g)\\in\\text{JOINT\\_PAIRS}\\):\n",
    "1. Let \\((x,y)\\) = values of \\((f,g)\\) in the test record.  \n",
    "2. Compute  \n",
    "   \\[\n",
    "     P_{\\text{synth}}\\bigl(f\\le x \\,\\wedge\\, g\\le y\\bigr),\n",
    "     \\quad\n",
    "     P_{\\text{test}}\\bigl(f\\le x \\,\\wedge\\, g\\le y\\bigr).\n",
    "   \\]\n",
    "3. Record the absolute difference.  \n",
    "4. Average over all pairs to get the **joint score**.\n",
    "\n",
    "### 3.4 Scoring & Thresholding  \n",
    "- **Per-record score** = joint score − marginal score.  \n",
    "- Collect scores for all test records: \\(\\{\\,\\text{score}(x_i)\\}\\).  \n",
    "- Choose a threshold \\(t\\) (default: median of all scores).  \n",
    "- **Predict**:  \n",
    "  \\[\n",
    "    \\hat{y}_i = \n",
    "    \\begin{cases}\n",
    "      1 & \\text{if }\\text{score}(x_i)\\ge t\\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "  \\]\n",
    "\n",
    "### 3.5 Evaluation Metrics  \n",
    "Compare \\(\\hat{y}_i\\) against ground-truth \\(y_i\\in\\{0,1\\}\\) using:\n",
    "- **Accuracy**  \n",
    "- **Precision**  \n",
    "- **Recall**  \n",
    "- **F1 Score**  \n",
    "- **AUC** (ROC Curve area)  \n",
    "- **ASR** (Attack Success Rate = fraction correct)\n",
    "\n",
    "## 4. Example Pseudocode\n",
    "\n",
    "```python\n",
    "for each test record x_i:\n",
    "    # Marginal\n",
    "    m_diffs = [ abs((synth[f] <= x_i[f]).mean() - (test[f] <= x_i[f]).mean())\n",
    "                for f in MARGINAL_COLUMNS ]\n",
    "    m_score = np.mean(m_diffs)\n",
    "\n",
    "    # Joint\n",
    "    j_diffs = [ abs(( (synth[f] <= x_i[f]) & (synth[g] <= x_i[g]) ).mean()\n",
    "                  - ( (test[f]  <= x_i[f]) & (test[g]  <= x_i[g]) ).mean() )\n",
    "                for (f,g) in JOINT_PAIRS ]\n",
    "    j_score = np.mean(j_diffs)\n",
    "\n",
    "    score_i = j_score - m_score\n",
    "    scores.append(score_i)\n",
    "\n",
    "# Threshold at median(scores), then compute metrics.\n",
    "```\n",
    "\n",
    "## 5. Practical Considerations\n",
    "\n",
    "* **Choice of Columns**\n",
    "  Marginal and joint sets should reflect features most likely to exhibit over-fitting (e.g., numerical lab results, correlated visit counts).\n",
    "\n",
    "* **Sensitivity to Threshold**\n",
    "  Median works well when the attack is balanced, but you may tune $t$ for a desired precision/recall tradeoff.\n",
    "\n",
    "* **Computational Cost**\n",
    "  Each per-record computation is $O(d\\,N)$ where $d$ is number of features/pairs and $N$ is dataset size. Caching CDF indicators or using binned histograms can speed this up.\n",
    "\n",
    "* **Advantages**\n",
    "\n",
    "  * Captures both marginal shifts and dependency changes.\n",
    "  * Simple: no need to train a separate classifier.\n",
    "\n",
    "* **Limitations**\n",
    "\n",
    "  * Relies on numeric ordering—categorical data must be encoded ordinally or via separate treatment.\n",
    "  * Less effective if the synthesizer preserves CDFs perfectly.\n"
   ],
   "id": "c290597d7f7fd11a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T10:25:50.015576Z",
     "start_time": "2025-07-24T10:25:48.913116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ─── Configuration ──────────────────────────────────────────────────────────────\n",
    "FOLDER_PATH     = \"dataset/MIA_SDG_Exercise\"\n",
    "SYNTHETIC_FILES = [f\"synthetic_data{i}.csv\" for i in range(1, 5)]\n",
    "TEST_FILES      = [\"test_data_with_outliers.csv\", \"test_data_wto_outliers.csv\"]\n",
    "LABEL_COLUMN    = \"is_member\"\n",
    "\n",
    "# Columns for the copula-based attack\n",
    "MARGINAL_COLUMNS = [\n",
    "    \"lab_test_count\",\n",
    "    \"diagnosis_count\",\n",
    "    \"stay_duration_days\",\n",
    "    \"medication_count\"\n",
    "]\n",
    "JOINT_PAIRS = [\n",
    "    (\"lab_test_count\",    \"diagnosis_count\"),\n",
    "    (\"lab_test_count\",    \"procedure_count\"),\n",
    "    (\"stay_duration_days\",\"inpatient_visits\")\n",
    "]\n",
    "\n",
    "\n",
    "def compute_copula_scores(\n",
    "    synth_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each test row:\n",
    "      - Marginal score: average |F_synth(col <= x_i) - F_test(col <= x_i)| over MARGINAL_COLUMNS\n",
    "      - Joint score:   average |P_synth(c1<=x_i, c2<=y_i) - P_test(c1<=x_i, c2<=y_i)| over JOINT_PAIRS\n",
    "      - Final score = joint_score - marginal_score\n",
    "    Returns an array of per-row scores.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    # Precompute nothing; we'll use vectorized means inside the loop\n",
    "    for _, row in test_df.iterrows():\n",
    "        # Marginal\n",
    "        marg_diffs = []\n",
    "        for col in MARGINAL_COLUMNS:\n",
    "            if col in synth_df.columns and col in test_df.columns:\n",
    "                x = row[col]\n",
    "                if pd.notna(x):\n",
    "                    diff = abs(\n",
    "                        (synth_df[col] <= x).mean()\n",
    "                      - (test_df[col]  <= x).mean()\n",
    "                    )\n",
    "                    marg_diffs.append(diff)\n",
    "        m_score = np.mean(marg_diffs) if marg_diffs else 0.0\n",
    "\n",
    "        # Joint\n",
    "        joint_diffs = []\n",
    "        for c1, c2 in JOINT_PAIRS:\n",
    "            if c1 in synth_df.columns and c2 in synth_df.columns and c1 in test_df.columns and c2 in test_df.columns:\n",
    "                x1, x2 = row[c1], row[c2]\n",
    "                if pd.notna(x1) and pd.notna(x2):\n",
    "                    s_prob = ((synth_df[c1] <= x1) & (synth_df[c2] <= x2)).mean()\n",
    "                    t_prob = ((test_df[c1]  <= x1) & (test_df[c2]  <= x2)).mean()\n",
    "                    joint_diffs.append(abs(s_prob - t_prob))\n",
    "        j_score = np.mean(joint_diffs) if joint_diffs else 0.0\n",
    "\n",
    "        scores.append(j_score - m_score)\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def evaluate_attack(\n",
    "    scores: np.ndarray,\n",
    "    labels: pd.Series,\n",
    "    threshold: float = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute Accuracy, Precision, Recall, F1, AUC, ASR from per-row scores\n",
    "    and true 'yes'/'no' labels. Threshold is median(scores) if None.\n",
    "    \"\"\"\n",
    "    y_scores = np.array(scores)\n",
    "    y_true   = np.array([1 if l == \"yes\" else 0 for l in labels])\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = np.median(y_scores)\n",
    "\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "    return {\n",
    "        \"Accuracy\" : accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\"   : recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\"       : f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC\"      : roc_auc_score(y_true, y_scores),\n",
    "        \"ASR\"      : float((y_pred == y_true).mean())\n",
    "    }\n",
    "\n",
    "\n",
    "def run_copula_attacks() -> dict:\n",
    "    \"\"\"\n",
    "    Runs the fixed copula-based (marginal + joint CDF) attack on all\n",
    "    synthetic/test dataset pairs. Returns a mapping \"synth vs test\" -> metrics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for synth_file in SYNTHETIC_FILES:\n",
    "        synth_path = os.path.join(FOLDER_PATH, synth_file)\n",
    "        synth_df   = pd.read_csv(synth_path)\n",
    "\n",
    "        for test_file in TEST_FILES:\n",
    "            test_path = os.path.join(FOLDER_PATH, test_file)\n",
    "            df        = pd.read_csv(test_path).dropna(subset=[LABEL_COLUMN])\n",
    "\n",
    "            key = f\"{synth_file} vs {test_file}\"\n",
    "            print(f\"\\n🔍 Running Copula Attack on: {key}\")\n",
    "\n",
    "            # Align columns and split off the label\n",
    "            common_cols  = [\n",
    "                c for c in df.columns\n",
    "                if c in synth_df.columns and c != LABEL_COLUMN\n",
    "            ]\n",
    "            subset       = df[common_cols + [LABEL_COLUMN]]\n",
    "            test_subset  = subset.dropna(subset=[LABEL_COLUMN])\n",
    "            labels       = test_subset[LABEL_COLUMN]\n",
    "            X_test       = test_subset.drop(columns=[LABEL_COLUMN])\n",
    "            X_synth      = synth_df[common_cols].copy()\n",
    "\n",
    "            # Compute per-row copula scores\n",
    "            scores = compute_copula_scores(X_synth, X_test)\n",
    "\n",
    "            # Evaluate and store\n",
    "            results[key] = evaluate_attack(scores, labels)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results: dict):\n",
    "    \"\"\"Nicely prints metrics for each dataset pair.\"\"\"\n",
    "    for pair, metrics in results.items():\n",
    "        print(f\"\\n==== Results for {pair} ====\")\n",
    "        for name, val in metrics.items():\n",
    "            print(f\"{name:>10}: {val:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    attack_results = run_copula_attacks()\n",
    "    print_results(attack_results)\n"
   ],
   "id": "c4f318cc8608db95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running Copula Attack on: synthetic_data1.csv vs test_data_with_outliers.csv\n",
      "\n",
      "🔍 Running Copula Attack on: synthetic_data1.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "🔍 Running Copula Attack on: synthetic_data2.csv vs test_data_with_outliers.csv\n",
      "\n",
      "🔍 Running Copula Attack on: synthetic_data2.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "🔍 Running Copula Attack on: synthetic_data3.csv vs test_data_with_outliers.csv\n",
      "\n",
      "🔍 Running Copula Attack on: synthetic_data3.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "🔍 Running Copula Attack on: synthetic_data4.csv vs test_data_with_outliers.csv\n",
      "\n",
      "🔍 Running Copula Attack on: synthetic_data4.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.5417\n",
      " Precision: 0.5417\n",
      "    Recall: 0.5417\n",
      "        F1: 0.5417\n",
      "       AUC: 0.5968\n",
      "       ASR: 0.5417\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.4500\n",
      " Precision: 0.4500\n",
      "    Recall: 0.4500\n",
      "        F1: 0.4500\n",
      "       AUC: 0.3975\n",
      "       ASR: 0.4500\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.5833\n",
      " Precision: 0.5833\n",
      "    Recall: 0.5833\n",
      "        F1: 0.5833\n",
      "       AUC: 0.6241\n",
      "       ASR: 0.5833\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.4500\n",
      " Precision: 0.4500\n",
      "    Recall: 0.4500\n",
      "        F1: 0.4500\n",
      "       AUC: 0.4925\n",
      "       ASR: 0.4500\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.4583\n",
      " Precision: 0.4583\n",
      "    Recall: 0.4583\n",
      "        F1: 0.4583\n",
      "       AUC: 0.4753\n",
      "       ASR: 0.4583\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 0.5000\n",
      "        F1: 0.5000\n",
      "       AUC: 0.4475\n",
      "       ASR: 0.5000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.5208\n",
      " Precision: 0.5208\n",
      "    Recall: 0.5208\n",
      "        F1: 0.5208\n",
      "       AUC: 0.5069\n",
      "       ASR: 0.5208\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.5000\n",
      " Precision: 0.5000\n",
      "    Recall: 0.5000\n",
      "        F1: 0.5000\n",
      "       AUC: 0.4850\n",
      "       ASR: 0.5000\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Attack 8 - Boundary Anomaly Attack\n",
    "\n",
    "## 1. Overview  \n",
    "The **Boundary Anomaly Attack** is a simple, data-driven membership inference attack that flags a test record as a “member” if any of its feature values lie **outside** the ranges or categories observed in a held-out subset of the real (training) data. It works in two phases:\n",
    "\n",
    "1. **Inference phase** (using only the real data):  \n",
    "   - Learn the **numeric bounds** (min/max) for each numeric feature.  \n",
    "   - Learn the **valid category set** for each categorical feature.\n",
    "\n",
    "2. **Scoring phase** (using both real and synthetic data):  \n",
    "   - For each record in the **test set**, count how many feature values violate the inferred bounds or categories.  \n",
    "   - Convert that count into a per-record **anomaly score** (fraction of features violating constraints).\n",
    "\n",
    "Records with a high anomaly score are more likely to have been **excluded** from the training set (non-member), while those with low or zero anomaly score are more likely to be **members**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data & Setup\n",
    "\n",
    "- **Synthetic datasets**: `synthetic_data1.csv` … `synthetic_data4.csv`  \n",
    "- **Test datasets**:  \n",
    "  - `test_data_with_outliers.csv` (outlier-focused samples)  \n",
    "  - `test_data_wto_outliers.csv` (random samples)  \n",
    "- Each test record has a label `is_member ∈ {yes, no}` indicating ground-truth membership in the training set.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Preprocessing (Inference Phase)\n",
    "\n",
    "1. **Filter real-members**  \n",
    "   - From the test set, select only those rows with `is_member == \"yes\"`.  \n",
    "   - Drop the `is_member` column.\n",
    "\n",
    "2. **Separate feature types**  \n",
    "   - **Numeric columns**: all columns detected as numbers.  \n",
    "   - **Categorical columns**: all columns detected as strings/objects.\n",
    "\n",
    "3. **Infer numeric bounds**  \n",
    "   ```python\n",
    "   bounds[col] = ( real_df[col].min(), real_df[col].max() )\n",
    "```\n",
    "\n",
    "for each numeric column.\n",
    "\n",
    "4. **Infer valid categories**\n",
    "\n",
    "   ```python\n",
    "   categories[col] = real_df[col].unique().tolist()\n",
    "   ```\n",
    "\n",
    "   for each categorical column.\n",
    "\n",
    "All of this runs **once per test file** (not per synthetic dataset), so the attack uses only the held-out real data to learn the “allowed” feature space.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Scoring Mechanism (Attack Phase)\n",
    "\n",
    "For each synthetic/test pairing:\n",
    "\n",
    "1. **Align columns**\n",
    "\n",
    "   * Identify the intersection of features present in both the synthetic data and the real-data schema used for inference.\n",
    "\n",
    "2. **Per-row anomaly score**\n",
    "   For each test record `row`:\n",
    "\n",
    "   1. Initialize `violations = 0` and `checks = 0`.\n",
    "   2. **Numeric features**\n",
    "\n",
    "      ```python\n",
    "      for col, (min_val, max_val) in bounds.items():\n",
    "          checks += 1\n",
    "          if row[col] < min_val or row[col] > max_val:\n",
    "              violations += 1\n",
    "      ```\n",
    "   3. **Categorical features**\n",
    "\n",
    "      ```python\n",
    "      for col, valid_vals in categories.items():\n",
    "          checks += 1\n",
    "          if row[col] not in valid_vals:\n",
    "              violations += 1\n",
    "      ```\n",
    "   4. **Normalize**\n",
    "\n",
    "      ```python\n",
    "      score = violations / checks\n",
    "      ```\n",
    "\n",
    "      – a value in \\[0, 1], where 0 means “fully within bounds” and 1 means “all features out of bounds.”\n",
    "\n",
    "3. **Collect** the resulting `score` for every record in the test set into a 1-D array.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Membership Decision & Evaluation\n",
    "\n",
    "1. **Thresholding**\n",
    "\n",
    "   * Choose a decision threshold (by default, the **median** of all anomaly scores).\n",
    "   * Predict `member (1)` if `score >= threshold`, else `non-member (0)`.\n",
    "\n",
    "2. **Metrics**\n",
    "   Compare predictions against the ground-truth `is_member` labels using:\n",
    "\n",
    "   * **Accuracy**\n",
    "   * **Precision**\n",
    "   * **Recall**\n",
    "   * **F1 score**\n",
    "   * **AUC (ROC)**\n",
    "   * **ASR** (Attack Success Rate = fraction of correct guesses)"
   ],
   "id": "a5707c3e1c2559cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T10:29:07.756770Z",
     "start_time": "2025-07-24T10:29:07.508022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ─── Configuration ──────────────────────────────────────────────────────────────\n",
    "FOLDER_PATH     = \"dataset/MIA_SDG_Exercise\"\n",
    "SYNTHETIC_FILES = [f\"synthetic_data{i}.csv\" for i in range(1, 5)]\n",
    "TEST_FILES      = [\"test_data_with_outliers.csv\", \"test_data_wto_outliers.csv\"]\n",
    "LABEL_COLUMN    = \"is_member\"\n",
    "\n",
    "\n",
    "# ─── Helper Functions ────────────────────────────────────────────────────────────\n",
    "def infer_boundaries(df: pd.DataFrame, numeric_cols: list[str]) -> dict:\n",
    "    \"\"\"Infer (min, max) bounds for each numeric column from df.\"\"\"\n",
    "    bounds = {}\n",
    "    for col in numeric_cols:\n",
    "        series = df[col].dropna().astype(float)\n",
    "        bounds[col] = (series.min(), series.max())\n",
    "    return bounds\n",
    "\n",
    "def infer_categories(df: pd.DataFrame, categorical_cols: list[str]) -> dict:\n",
    "    \"\"\"Infer valid categories for each categorical column from df.\"\"\"\n",
    "    cats = {}\n",
    "    for col in categorical_cols:\n",
    "        cats[col] = df[col].dropna().unique().tolist()\n",
    "    return cats\n",
    "\n",
    "def boundary_violation_score(\n",
    "    row: dict,\n",
    "    bounds: dict,\n",
    "    categories: dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Given a row (as dict), plus numeric bounds and valid category lists,\n",
    "    return the fraction of features that violate those constraints.\n",
    "    \"\"\"\n",
    "    violations = 0.0\n",
    "    total_checks = 0\n",
    "\n",
    "    # Numeric violations\n",
    "    for col, (min_val, max_val) in bounds.items():\n",
    "        total_checks += 1\n",
    "        val = row.get(col, None)\n",
    "        if val is None or pd.isna(val) or not (min_val <= float(val) <= max_val):\n",
    "            violations += 1.0\n",
    "\n",
    "    # Categorical violations\n",
    "    for col, valid_vals in categories.items():\n",
    "        total_checks += 1\n",
    "        val = row.get(col, None)\n",
    "        if val is None or pd.isna(val) or val not in valid_vals:\n",
    "            violations += 1.0\n",
    "\n",
    "    # Return average violation rate (0.0 = no violations, 1.0 = all violate)\n",
    "    return violations / total_checks if total_checks > 0 else 0.0\n",
    "\n",
    "def evaluate_attack(\n",
    "    scores: np.ndarray,\n",
    "    labels: list[str],\n",
    "    threshold: float = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute Accuracy, Precision, Recall, F1, AUC, and ASR for\n",
    "    binary labels ('yes'/'no') given per-row anomaly scores.\n",
    "    \"\"\"\n",
    "    y_scores = np.array(scores)\n",
    "    y_true   = np.array([1 if l == \"yes\" else 0 for l in labels])\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = np.median(y_scores)\n",
    "\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\":  accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\":    recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\":        f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC\":       roc_auc_score(y_true, y_scores),\n",
    "        \"ASR\":       float((y_pred == y_true).mean())\n",
    "    }\n",
    "\n",
    "\n",
    "# ─── Main Attack Runner ─────────────────────────────────────────────────────────\n",
    "def run_boundary_anomaly_attack() -> dict:\n",
    "    \"\"\"\n",
    "    For each test set, infer real-data boundaries & categories once.\n",
    "    Then for each synthetic dataset, score all test rows for boundary\n",
    "    violations (restricted to features shared by real & synth) and\n",
    "    evaluate membership inference performance.\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "\n",
    "    for test_file in TEST_FILES:\n",
    "        # Load and filter test data\n",
    "        test_path = os.path.join(FOLDER_PATH, test_file)\n",
    "        df_test   = pd.read_csv(test_path).dropna(subset=[LABEL_COLUMN])\n",
    "        labels    = df_test[LABEL_COLUMN].tolist()\n",
    "\n",
    "        # Real-members subset for inferring bounds/categories\n",
    "        real_df = df_test[df_test[LABEL_COLUMN] == \"yes\"].drop(columns=[LABEL_COLUMN])\n",
    "\n",
    "        # Global numeric/categorical columns from real data\n",
    "        numeric_cols     = real_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_cols = real_df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "        # Infer once per test set\n",
    "        bounds     = infer_boundaries(real_df, numeric_cols)\n",
    "        categories = infer_categories(real_df, categorical_cols)\n",
    "\n",
    "        for synth_file in SYNTHETIC_FILES:\n",
    "            synth_path = os.path.join(FOLDER_PATH, synth_file)\n",
    "            df_synth   = pd.read_csv(synth_path)\n",
    "\n",
    "            # Only consider features present in both real_df and synthetic\n",
    "            common_cols = [c for c in real_df.columns if c in df_synth.columns]\n",
    "\n",
    "            # Restrict bounds/categories to these shared features\n",
    "            bounds_sub     = {c: bounds[c] for c in numeric_cols     if c in common_cols}\n",
    "            categories_sub = {c: categories[c] for c in categorical_cols if c in common_cols}\n",
    "\n",
    "            print(f\"🔍 Running Boundary Anomaly Attack on: {synth_file} vs {test_file}\")\n",
    "\n",
    "            # Compute per-row scores\n",
    "            scores = []\n",
    "            for _, row in df_test.iterrows():\n",
    "                row_dict = row.to_dict()\n",
    "                scores.append(boundary_violation_score(row_dict, bounds_sub, categories_sub))\n",
    "\n",
    "            # Evaluate\n",
    "            key = f\"{synth_file} vs {test_file}\"\n",
    "            all_results[key] = evaluate_attack(np.array(scores), labels)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def print_results(results: dict):\n",
    "    \"\"\"Print the metrics for each dataset pair.\"\"\"\n",
    "    for pair, metrics in results.items():\n",
    "        print(f\"\\n==== Results for {pair} ====\")\n",
    "        for name, val in metrics.items():\n",
    "            print(f\"{name:>10}: {val:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    attack_results = run_boundary_anomaly_attack()\n",
    "    print_results(attack_results)\n"
   ],
   "id": "55de52dc7202e50c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Boundary Anomaly Attack on: synthetic_data1.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Boundary Anomaly Attack on: synthetic_data2.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Boundary Anomaly Attack on: synthetic_data3.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Boundary Anomaly Attack on: synthetic_data4.csv vs test_data_with_outliers.csv\n",
      "🔍 Running Boundary Anomaly Attack on: synthetic_data1.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Boundary Anomaly Attack on: synthetic_data2.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Boundary Anomaly Attack on: synthetic_data3.csv vs test_data_wto_outliers.csv\n",
      "🔍 Running Boundary Anomaly Attack on: synthetic_data4.csv vs test_data_wto_outliers.csv\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.2396\n",
      " Precision: 0.2642\n",
      "    Recall: 0.2917\n",
      "        F1: 0.2772\n",
      "       AUC: 0.1508\n",
      "       ASR: 0.2396\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.2917\n",
      " Precision: 0.3649\n",
      "    Recall: 0.5625\n",
      "        F1: 0.4426\n",
      "       AUC: 0.0888\n",
      "       ASR: 0.2917\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.2396\n",
      " Precision: 0.2642\n",
      "    Recall: 0.2917\n",
      "        F1: 0.2772\n",
      "       AUC: 0.1508\n",
      "       ASR: 0.2396\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_with_outliers.csv ====\n",
      "  Accuracy: 0.2396\n",
      " Precision: 0.2642\n",
      "    Recall: 0.2917\n",
      "        F1: 0.2772\n",
      "       AUC: 0.1508\n",
      "       ASR: 0.2396\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data1.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.1000\n",
      " Precision: 0.1364\n",
      "    Recall: 0.1500\n",
      "        F1: 0.1429\n",
      "       AUC: 0.0175\n",
      "       ASR: 0.1000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data2.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.1000\n",
      " Precision: 0.1364\n",
      "    Recall: 0.1500\n",
      "        F1: 0.1429\n",
      "       AUC: 0.0163\n",
      "       ASR: 0.1000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data3.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.1000\n",
      " Precision: 0.1364\n",
      "    Recall: 0.1500\n",
      "        F1: 0.1429\n",
      "       AUC: 0.0175\n",
      "       ASR: 0.1000\n",
      "------------------------------\n",
      "\n",
      "==== Results for synthetic_data4.csv vs test_data_wto_outliers.csv ====\n",
      "  Accuracy: 0.1000\n",
      " Precision: 0.1364\n",
      "    Recall: 0.1500\n",
      "        F1: 0.1429\n",
      "       AUC: 0.0175\n",
      "       ASR: 0.1000\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4d773e56e0ec9e9e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
